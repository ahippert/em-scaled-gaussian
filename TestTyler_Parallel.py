# -*- coding: utf-8 -*
##############################################################################
# A file for comparing the estimation error (natural distance) evolution as
# the number of dates grows
# Authored by Ammar Mian, 13/05/2019
# e-mail: ammar.mian@centralesupelec.fr
##############################################################################
# Copyright 2019 @CentraleSupelec
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
##############################################################################
import sys
import numpy as np
import scipy as sp
import scipy.special
import warnings
from matrix_operators import sqrtm, invsqrtm, expm, logm, powm, inv
import time
from joblib import Parallel, delayed
import matplotlib.pyplot as plt
from tqdm import tqdm
#import tikzplotlib
import warnings
warnings.simplefilter('once', UserWarning)
#from estim_param import estim_block_normalizedet

# Just for having a nice color style when plotting
# Can be commented
#import seaborn as sns
#sns.set_style("darkgrid")

# ---------------------------------------------------------------------------------------------------------------
# Definition of functions for Generation of Data
# ---------------------------------------------------------------------------------------------------------------
def ToeplitzMatrix(rho, p):
    """ A function that computes a Hermitian semi-positive matrix.
            Inputs:
                * rho = a scalar
                * p = size of matrix
            Outputs:
                * the matrix """

    return sp.linalg.toeplitz(np.power(rho, np.arange(0, p)))

def multivariate_complex_normal_samples(mean, covariance, N, pseudo_covariance=0):
    """ A function to generate multivariate complex normal vectos as described in:
        Picinbono, B. (1996). Second-order complex random vectors and normal
        distributions. IEEE Transactions on Signal Processing, 44(10), 2637â€“2640.
        Inputs:
            * mean = vector of size p, mean of the distribution
            * covariance = the covariance matrix of size p*p(Gamma in the paper)
            * pseudo_covariance = the pseudo-covariance of size p*p (C in the paper)
                for a circular distribution omit the parameter
            * N = number of Samples
        Outputs:
            * Z = Samples from the complex Normal multivariate distribution, size p*N"""

    (p, p) = covariance.shape
    Gamma = covariance
    C = pseudo_covariance

    # Computing elements of matrix Gamma_2r
    Gamma_x = 0.5 * np.real(Gamma + C)
    Gamma_xy = 0.5 * np.imag(-Gamma + C)
    Gamma_yx = 0.5 * np.imag(Gamma + C)
    Gamma_y = 0.5 * np.real(Gamma - C)

    # Matrix Gamma_2r as a block matrix
    Gamma_2r = np.block([[Gamma_x, Gamma_xy], [Gamma_yx, Gamma_y]])

    # Generating the real part and imaginary part
    mu = np.hstack((mean.real, mean.imag))
    v = np.random.multivariate_normal(mu, Gamma_2r, N).T
    X = v[0:p, :]
    Y = v[p:, :]
    return X + 1j * Y

def generate_data_compound_Gaussian(ğ›•, p, N, ğ›, ğšº, pseudo_ğšº):
    # Re-seed the random number generator
    #np.random.seed()
    ğ— = np.empty((p,N), dtype=complex)
    ğ— = np.sqrt(ğ›•)[None,:] * multivariate_complex_normal_samples(ğ›, ğšº, N, pseudo_ğšº)
    return ğ—

# ---------------------------------------------------------------------------------------------------------------
# Definition of functions for Covariance Estimation
# ---------------------------------------------------------------------------------------------------------------

def SCM(x, *args):
    """ A function that computes the SCM for covariance matrix estimation
            Inputs:
                * x = a matrix of size p*N with each observation along column dimension
            Outputs:
                * Sigma = the estimate"""

    (p, N) = x.shape
    return (x @ x.conj().T) / N

def tyler_estimator_covariance_normalisedet(ğ—, tol=0.001, iter_max=20, init=None):
    """ A function that computes the Tyler Fixed Point Estimator for covariance matrix estimation
                    and normalisation by determinant
                    Inputs:
                        * ğ— = a matrix of size p*N with each observation along column dimension
                        * tol = tolerance for convergence of estimator
                        * iter_max = number of maximum iterations
                        * init = Initialisation point of the fixed-point, default is identity matrix
                    Outputs:
                        * ğšº = the estimate
                        * Î´ = the final distance between two iterations
                        * iteration = number of iterations til convergence """

    # Initialisation
    (p,N) = ğ—.shape
    Î´ = np.inf # Distance between two iterations
    if init is None:
        ğšº = np.eye(p) # Initialise estimate to identity
    else:
        ğšº = init
    iteration = 0

    Ï„=np.zeros((p,N))
    # Recursive algorithm
    while (Î´>tol) and (iteration<iter_max):

        # Computing expression of Tyler estimator (with matrix multiplication)
        v=np.linalg.inv(np.linalg.cholesky(ğšº))@ğ—
        a=np.mean(v*v.conj(),axis=0)

        Ï„[0:p,:] = np.sqrt(np.real(a))
        ğ—_bis = ğ— / Ï„
        ğšº_new = (1/N) * ğ—_bis@ğ—_bis.conj().T

        # Imposing det constraint: det(ğšº) = 1 DOT NOT WORK HERE
        # ğšº = ğšº/(np.linalg.det(ğšº)**(1/p))

        # Condition for stopping
        Î´ = np.linalg.norm(ğšº_new - ğšº, 'fro') / np.linalg.norm(ğšº, 'fro')
        iteration = iteration + 1

        # Updating ğšº
        ğšº = ğšº_new

    # Calcul textures
    ğ›• = np.zeros(N)
    v = np.linalg.inv(np.linalg.cholesky(ğšº))@ğ—
    a = np.mean(v*v.conj(),axis=0)
    ğ›• = np.real(a)
    ğ›• = ğ›•*(np.linalg.det(ğšº)**(1/p))

    # Imposing det constraint: det(ğšº) = 1
    ğšº = ğšº/(np.linalg.det(ğšº)**(1/p))

    return ğšº, ğ›•

# ---------------------------------------------------------------------------------------------------------------
# Definition of functions for Computing Distance
# ---------------------------------------------------------------------------------------------------------------

def Î´_ğ’®â„‹plusplus(ğšº_0, ğšº_1):
    """ Fonction for computing the Riemannian distance between two PDH matrices
        ------------------------------------------------------------------------
        Inputs:
        --------
            * ğšº_0 = PDH matrix of dimension p
            * ğšº_1 = PDH matrix of dimension p

        Outputs:
        ---------
            * Î´ = the distance
        """

    isqrtmğšº_0 = invsqrtm(ğšº_0)
    Î´ = np.linalg.norm( logm( isqrtmğšº_0 @ ğšº_1 @ isqrtmğšº_0 ), 'fro') # A changer
    return Î´

def Î´_â„›plusplus(ğ›•_0, ğ›•_1):
    """ Fonction for computing the Riemannian distance between two texture parameters
        ------------------------------------------------------------------------------
        Inputs:
        --------
            * ğ›•_0 = vector in â„›+^N
            * ğ›•_1 = vector in â„›+^N

        Outputs:
        ---------
            * Î´ = the distance
        """

    Î´ = np.linalg.norm( np.log(ğ›•_1/ğ›•_0), 2)
    return Î´


def Î´_ğ“œpn(ğšº_0, ğšº_1, ğ›•_0, ğ›•_1):
    """ Fonction for computing the Riemannian distance between two parameters in manifold
        PDH x {R+}^N
        ----------------------------------------------------------------------------------
        Inputs:
        --------
            * ğšº_0 = PDH matrix of dimension p
            * ğšº_1 = PDH matrix of dimension p
            * ğ›•_0 = vector in â„›+^N
            * ğ›•_1 = vector in â„›+^N

        Outputs:
        ---------
            * Î´ = the distance
        """

    Î´2 = Î´_ğ’®â„‹plusplus(ğšº_0, ğšº_1)**2 + Î´_â„›plusplus(ğ›•_0, ğ›•_1)**2
    return np.sqrt(Î´2)

def Î´_ğ“œp1(ğšº_0, ğšº_1, Ï„_0, Ï„_1):
    """ Fonction for computing the Riemannian distance between two parameters in manifold
        PDH x R+
        ----------------------------------------------------------------------------------
        Inputs:
        --------
            * ğšº_0 = PDH matrix of dimension p
            * ğšº_1 = PDH matrix of dimension p
            * Ï„_0 = scalar in â„›+
            * Ï„_1 = scalar in â„›+

        Outputs:
        ---------
            * Î´ = the distance
        """

    Î´2 = Î´_ğ’®â„‹plusplus(ğšº_0, ğšº_1)**2 + (Ï„_0-Ï„_1)**2
    return np.sqrt(Î´2)

# ---------------------------------------------------------------------------------------------------------------
# Definition of functions for Monte - Carlo
# ---------------------------------------------------------------------------------------------------------------

def one_monte_carlo(trial_no,ğ›•, p, n, ğ›, ğšº, pseudo_ğšº):

        np.random.seed(trial_no)

        ğ— = generate_data_compound_Gaussian(ğ›•, p, n, ğ›, ğšº, pseudo_ğšº)

        m = 9
        r = n-10
        X_obs = X[:,:r]

        #ğšº_SCM = SCM(ğ—)
        #ğšº_SCM = ğšº_SCM / (np.linalg.det(ğšº_SCM)**(1/p))                       # Normalise shape matrix by determinant
        ğšº_Tyl_full, ğ›•_Tyl_full = tyler_estimator_covariance_normalisedet(X)
        ğšº_Tyl_obs, ğ›•_Tyl_obs = tyler_estimator_covariance_normalisedet(X_obs)
        ğšº_EM, ğ›•_EM, e, ll = estim_block_normalizedet(ğ—, m, r, tol=1e-6, iter_max=100)

        # Î´_ğšº_SCM = Î´_ğ’®â„‹plusplus(ğšº, ğšº_SCM) # Natural distance to true value, only shape matrix
        Î´_ğšº_Tyl_full = Î´_ğ’®â„‹plusplus(ğšº, ğšº_Tyl_full) # Natural distance to true value, only shape matrix
        Î´_ğšº_Tyl_obs = Î´_ğ’®â„‹plusplus(ğšº, ğšº_Tyl_obs) # Natural distance to true value, only shape matrix
        Î´_Î¸_Tyl = 0#Î´_ğ“œp1(ğšº, ğšº_Tyl, np.mean(ğ›•), np.mean(ğ›•_Tyl))
        Î´_ğ›•_Tyl = 0#Î´_â„›plusplus(ğ›•, ğ›•_Tyl)
        Î´_mean_ğ›•_Tyl = 0#(np.mean(ğ›•)-np.mean(ğ›•_Tyl))**2
        Î´_ğšº_EM = Î´_ğ’®â„‹plusplus(ğšº, ğšº_EM) # Natural distance to true value, only shape matrix

        return [Î´_ğšº_Tyl_full, Î´_ğšº_Tyl_obs, Î´_Î¸_Tyl, Î´_ğ›•_Tyl, Î´_mean_ğ›•_Tyl, Î´_ğšº_EM]

def parallel_monte_carlo(ğ›•, p, n, ğ›, ğšº, pseudo_ğšº, number_of_threads, number_of_trials, Multi):

    # Looping on Monte Carlo Trials
    if Multi:
        results_parallel = Parallel(n_jobs=number_of_threads)(delayed(one_monte_carlo)(iMC,ğ›•, p, n, ğ›, ğšº, pseudo_ğšº) for iMC in range(number_of_trials))
        results_parallel = np.array(results_parallel)
        Î´_ğšº_SCM = np.mean(results_parallel[:,0], axis=0)
        Î´_ğšº_Tyl = np.mean(results_parallel[:,1], axis=0)
        Î´_Î¸_Tyl = np.mean(results_parallel[:,2], axis=0)
        Î´_ğ›•_Tyl = np.mean(results_parallel[:,3], axis=0)
        Î´_mean_ğ›•_Tyl = np.mean(results_parallel[:,4], axis=0)
        Î´_EM = np.mean(results_parallel[:,5], axis=0)
        return Î´_ğšº_SCM, Î´_ğšº_Tyl, Î´_Î¸_Tyl, Î´_ğ›•_Tyl, Î´_mean_ğ›•_Tyl, Î´_EM
    else:
        # Results container
        results = []
        for iMC in range(number_of_trials):
            results.append(one_monte_carlo(iMC,ğ›•, p, n, ğ›, ğšº, pseudo_ğšº))

        results = np.array(results)
        Î´_ğšº_SCM = np.mean(results[:,0], axis=0)
        Î´_ğšº_Tyl = np.mean(results[:,1], axis=0)
        Î´_Î¸_Tyl = np.mean(results[:,2], axis=0)
        Î´_ğ›•_Tyl = np.mean(results[:,3], axis=0)
        Î´_mean_ğ›•_Tyl = np.mean(results[:,4], axis=0)
        Î´_EM = np.mean(results_parallel[:,5], axis=0)
        return Î´_ğšº_SCM, Î´_ğšº_Tyl, Î´_Î¸_Tyl, Î´_ğ›•_Tyl, Î´_mean_ğ›•_Tyl, Î´_EM

# ---------------------------------------------------------------------------------------------------------------
# Definition of Program Principal
# ---------------------------------------------------------------------------------------------------------------

if __name__ == '__main__':

    __spec__ = "ModuleSpec(name='builtins', loader=<class '_frozen_importlib.BuiltinImporter'>)"

    # ---------------------------------------------------------------------------------------------------------------
    # Simulation Parameters
    # ---------------------------------------------------------------------------------------------------------------
    number_of_threads = -1                                  # to use the maximum number of threads : -1
    Multi = True
    p = 10                                                  # Dimension of data
    n_vec = np.unique(np.logspace(1.8,2.5,6).astype(int))  # Number of dates
    number_of_trials = 1                               # Number of trials for each point of the MSE
    ğ› = np.zeros(p)                                         # Mean of Gaussian distribution
    pseudo_ğšº = 0                                            # Pseudo-covariance of Gaussian distribution
    Ï = 0.999 * (1+1j)/np.sqrt(2)                           # Toeplitz coefficient for shape matrix
    ğšº = ToeplitzMatrix(Ï, p)                                # Toeplitz shape matrix
    ğšº = ğšº / (np.linalg.det(ğšº)**(1/p))                       # Normalise shape matrix by determinant
    Î± = 1                                                # Shape parameter of Gamma texture
    Î² = 1/Î±                                                 # Scale parameter of Gamma texture

    # ---------------------------------------------------------------------------------------------------------------
    # Doing estimation for an increasing T and saving the natural distance to true value
    # ---------------------------------------------------------------------------------------------------------------
    # print( '|ï¿£ï¿£ï¿£ï¿£ï¿£ï¿£ï¿£ï¿£ï¿£ï¿£|')
    # print( '|   Launching      |')
    # print( '|   Monte Carlo    |')
    # print( '|   simulation     |' )
    # print( '|ï¼¿ï¼¿ï¼¿ï¼¿ï¼¿ï¼¿ï¼¿ï¼¿ï¼¿ï¼¿|')
    # print( ' (\__/) ||')
    # print( ' (â€¢ã……â€¢) || ')
    # print( ' / ã€€ ã¥')
    print(u"Parameters: p=%d, n=%s, rho=%.2f+i*%.2f, alpha=%.2f, beta=%.2f" % (p,n_vec,Ï.real,Ï.imag,Î±,Î²))
    t_beginning = time.time()

    # Distance containers
    Î´_ğšº_SCM_container = np.zeros(len(n_vec))
    Î´_ğšº_Tyl_container = np.zeros(len(n_vec))
    Î´_Î¸_Tyl_container = np.zeros(len(n_vec))
    Î´_ğ›•_Tyl_container = np.zeros(len(n_vec))
    Î´_mean_ğ›•_Tyl_container = np.zeros(len(n_vec))
    Î´_EM_container = np.zeros(len(n_vec))

    for i_n, n in enumerate(tqdm(n_vec)):
        # generation textures
        ğ›• = np.random.gamma(Î±, Î², n_vec[i_n])
        Î´_ğšº_SCM_container[i_n], Î´_ğšº_Tyl_container[i_n], Î´_Î¸_Tyl_container[i_n], Î´_ğ›•_Tyl_container[i_n], Î´_mean_ğ›•_Tyl_container[i_n], Î´_EM_container[i_n]  = parallel_monte_carlo(ğ›•, p, n_vec[i_n], ğ›, ğšº, pseudo_ğšº, number_of_threads, number_of_trials, Multi)

    print('Done in %f s'%(time.time()-t_beginning))

    # ---------------------------------------------------------------------------------------------------------------
    # Plotting using Matplotlib
    # ---------------------------------------------------------------------------------------------------------------
    markers = ['o', 's' , '*']#, '8', 'P', 'D', 'X']

    # Natural distance to true value, only texture
    # plt.figure(figsize=(16, 7), dpi=80, facecolor='w')
    plt.figure()
    # plt.subplot(1, 3, 1)
    plt.loglog(n_vec, Î´_ğšº_SCM_container, marker=markers[0], label='SCM')
    plt.loglog(n_vec, Î´_ğšº_Tyl_container, marker=markers[1], label='Tyler')
    plt.loglog(n_vec, Î´_EM_container, marker=markers[2], label='EM')
    plt.xlabel(r'$n$')
    plt.ylabel(r'$\delta_{\mathcal{SH}_{++}}\left( \mathbf{\xi}, \hat{\mathbf{\xi}} \right)$')
    plt.legend()
    plt.title(r"Parameters: $p=%d$, $\rho=%.2f+i*%.2f$, $\alpha=%.2f$, $\beta=%.2f$" % (p,Ï.real,Ï.imag,Î±,Î²))

    plt.figure()
    # plt.subplot(1, 3, 1)
    plt.loglog(n_vec, Î´_Î¸_Tyl_container, marker=markers[2], label='Tyler Covar + Mean Textures')
    plt.xlabel(r'$n$')
    plt.ylabel(r'$\delta_{\mathcal{M}_{p,n}}\left( Î¸, \hat{Î¸} \right)$')
    plt.legend()
    plt.title(r"Parameters: $p=%d$, $\rho=%.2f+i*%.2f$, $\alpha=%.2f$, $\beta=%.2f$" % (p,Ï.real,Ï.imag,Î±,Î²))

    plt.figure()
    # plt.subplot(1, 3, 1)
    plt.loglog(n_vec, Î´_ğ›•_Tyl_container, marker=markers[2], label='Tyler Textures')
    plt.xlabel(r'$n$')
    plt.ylabel(r'$\delta_{\mathcal{R}_{n}}\left( ğ›•, \hat{ğ›•} \right)$')
    plt.legend()
    plt.title(r"Parameters: $p=%d$, $\rho=%.2f+i*%.2f$, $\alpha=%.2f$, $\beta=%.2f$" % (p,Ï.real,Ï.imag,Î±,Î²))

    plt.figure()
    # plt.subplot(1, 3, 1)
    plt.loglog(n_vec, Î´_mean_ğ›•_Tyl_container, marker=markers[2], label='Mean Tyler Textures')
    plt.xlabel(r'$n$')
    plt.ylabel(r'$\delta_{\mathcal{R}}\left( E(ğ›•), \hat{E(ğ›•)} \right)$')
    plt.legend()
    plt.title(r"Parameters: $p=%d$, $\rho=%.2f+i*%.2f$, $\alpha=%.2f$, $\beta=%.2f$" % (p,Ï.real,Ï.imag,Î±,Î²))

    #tikzplotlib.save('Optimization_H0_scale.tex')
    plt.show()
